{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The below cell will expand the notebook width to the (almost - 95%) full width of the browser_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "document.getElementById('notebook-container').style.width = '95%'"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "document.getElementById('notebook-container').style.width = '95%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the path to the slack-pack/code/ folder in order to be able to import nlp module\n",
    "import sys, os\n",
    "\n",
    "NLP_PATH = '/'.join(os.path.abspath('.').split('/')[:-1]) + '/'\n",
    "sys.path.append(NLP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nlp.text import extractor as xt\n",
    "\n",
    "from nlp.geometry import repr as gr\n",
    "from nlp.geometry import dist as gd\n",
    "from nlp.grammar import tokenizer as gt\n",
    "from nlp.text import window as gw\n",
    "\n",
    "from nlp.models import similarity_calculation as gsc\n",
    "from nlp.models import message_classification as gmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different representations\n",
    "\n",
    "We need to load the different representations (we will use `nlp.geometry.repr.GloVe` class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.6B.100d.txt', 'glove.6B.300d.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.list_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 524 ms, total: 13.1 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the GloVe representation\n",
    "glove100_rep = gr.GloVe('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 s, sys: 1.28 s, total: 34.2 s\n",
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the GloVe representation\n",
    "glove300_rep = gr.GloVe('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance function\n",
    "\n",
    "The following function defines a distance between to texts (it first cleans them using `nlp.grammar.tokenizer.SimpleCleaner`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean = gt.SimpleCleaner()\n",
    "\n",
    "def dist_m2m(m1, m2):\n",
    "    # tokenize\n",
    "    text1 = clean(m1.lower())\n",
    "    text2 = clean(m2.lower())\n",
    "\n",
    "    # get geometric representation\n",
    "    rep1 = glove100_rep(text1)\n",
    "    rep2 = glove100_rep(text2)\n",
    "    \n",
    "    return gd.cosine(rep1, rep2)\n",
    "\n",
    "def dist_m2m_300(m1, m2):\n",
    "    # tokenize\n",
    "    text1 = clean(m1.lower())\n",
    "    text2 = clean(m2.lower())\n",
    "\n",
    "    # get geometric representation\n",
    "    rep1 = glove300_rep(text1)\n",
    "    rep2 = glove300_rep(text2)\n",
    "    \n",
    "    return gd.cosine(rep1, rep2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions for inspecting the outputted window (topic list)\n",
    "\n",
    "With `inspect_window` we get a list of the topics and the #messages in each\n",
    "\n",
    "With `print_topic` we get all the messages in the given topic, along with the reason why they were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspect_window(window):\n",
    "    print( 'Window has #{} topics\\n'.format( len(window) ) )\n",
    "    \n",
    "    print( 'Topic length report:' )\n",
    "    for i, tpc in enumerate(window):\n",
    "        print( '  Topic #{:>2}  --> size: {:<3}'.format(i, len(tpc)) )\n",
    "\n",
    "def print_topic(topic):\n",
    "    for i,(m,r) in enumerate(topic):\n",
    "        print '{} -- {}\\n\\t\\033[33m{}\\033[0m\\n\\n'.format(i,r,m.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classifier\n",
    "\n",
    "The main classifying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_stream(message_stream, distance=dist_m2m, max_messages=20,\n",
    "                    low_threshold=.4, high_threshold=.7, low_step=.05, high_step=.02, verbose=True):\n",
    "    topics = []\n",
    "    for m, msg in enumerate(message_stream):\n",
    "        if m > max_messages:\n",
    "            m -= 1\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print '#{:>3}\\033[33m ==> {}\\033[0m'.format(m, msg.text.encode('ascii', 'ignore'))\n",
    "\n",
    "        if len(topics) == 0:\n",
    "            topics.insert(0, [(msg, 'First message')] )\n",
    "            if verbose:\n",
    "                print '\\t First message (new 0)\\n'\n",
    "\n",
    "        else:\n",
    "            # We will sequentially try to append to each topic ...\n",
    "            #    as time goes by it is harder to append to a topic\n",
    "\n",
    "            low_th = low_threshold\n",
    "            high_th = high_threshold\n",
    "            topic_scores = []  # in case no topic is close\n",
    "\n",
    "            for t in xrange(len(topics)):\n",
    "                tp_len = len(topics[t])\n",
    "                distances = map(lambda x: distance(msg.text, x[0].text), topics[t])\n",
    "\n",
    "                # Assign a non-linear score (very close messages score higher)\n",
    "                score = sum([ 0 if d < low_th else 1 if d < high_th else 3 for d in distances ])\n",
    "\n",
    "                # Very large topics (> 10) should be harder to append to,\n",
    "                #   since the odds of a casual match are higher\n",
    "                if (tp_len < 3):\n",
    "                    if (score > 0):\n",
    "                        reason = 'len({}) < 3 and distances({})'.format(tp_len, distances)\n",
    "                        _topic = topics.pop(t)  # pop from topic queue\n",
    "                        _topic.append( (msg, reason) )\n",
    "                        topics.insert(0, _topic)  # append to first topic\n",
    "                        if verbose:\n",
    "                            print '\\t inserted to #{} : {}\\n'.format(t, reason)\n",
    "                        break\n",
    "\n",
    "                elif (tp_len < 10):\n",
    "                    if (score > (tp_len - (2 - tp_len/15.) )):\n",
    "                        reason = 'len({}) < 10 and distances({})'.format(tp_len, distances)\n",
    "                        _topic = topics.pop(t)  # pop from topic queue\n",
    "                        _topic.append( (msg, 'len({}) < 10 and distances({})'.format(tp_len, distances)) )\n",
    "                        topics.insert(0, _topic)  # append to first topic\n",
    "                        if verbose:\n",
    "                            print '\\t inserted to #{} : {}\\n'.format(t, reason)\n",
    "                        break\n",
    "\n",
    "                elif (tp_len > 10):\n",
    "                    if (score > tp_len*1.5):\n",
    "                        reason = 'len({}) > 10 and distances({})'.format(tp_len, distances)\n",
    "                        _topic = topics.pop(t)  # pop from topic queue\n",
    "                        _topic.append( (msg, 'len({}) > 10 and distances({})'.format(tp_len, distances)) )\n",
    "                        topics.insert(0, _topic)  # append to first topic\n",
    "                        if verbose:\n",
    "                            print '\\t inserted to #{} : {}\\n'.format(t, reason)\n",
    "                        break\n",
    "\n",
    "                topic_scores.append( (tp_len,score) )  # append score to topic_scores\n",
    "\n",
    "                # else try with next topic --> harder\n",
    "                low_th += low_step if low_th+low_step < high_th else high_step\n",
    "                high_th += high_step\n",
    "            else:\n",
    "                # If no topic was suitable --> Start new topic\n",
    "                topics.insert(0, [(msg, 'No similar topics (to 0) scores:({})'.format(topic_scores))] )\n",
    "                if verbose:\n",
    "                    print '\\t No similar topics (new 0) scores:({})\\n'.format(topic_scores)\n",
    "\n",
    "    print '... Done, processed {} messages'.format(m)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the extractor (JSON or Cassandra)\n",
    "awwdb = xt.CassandraExtractor(cluster_ips=['54.175.189.47'],\n",
    "                              session_keyspace='test_keyspace',\n",
    "                              table_name='awaybot_messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'architecture',\n",
       " u'bot-sandbox',\n",
       " u'class-deliverables',\n",
       " u'code-documentation',\n",
       " u'data',\n",
       " u'general',\n",
       " u'github-repo',\n",
       " u'name-selection',\n",
       " u'next-meeting',\n",
       " u'nlp-methodology',\n",
       " u'tech-stuff'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "awwdb.list_channels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0\u001b[33m ==> are you using outgoing webhooks for pushing the messages out?\u001b[0m\n",
      "\t First message (new 0)\n",
      "\n",
      "#  1\u001b[33m ==> No, the tutorial is using a websocket connection\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(1, 0)])\n",
      "\n",
      "#  2\u001b[33m ==> which I'm not really familiar with\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(1, 0), (1, 0)])\n",
      "\n",
      "#  3\u001b[33m ==> I'm reading up on NLP meanwhile\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.55818406822222211])\n",
      "\n",
      "#  4\u001b[33m ==> :slightly_smiling_face: this is really cool stuff\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(2, 0), (1, 0), (1, 0)])\n",
      "\n",
      "#  5\u001b[33m ==> Nice. Hopefully NLP class will give you some good resources as well.\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.54949126842986729])\n",
      "\n",
      "#  6\u001b[33m ==> just googling around I found this. I don't really understand it, but thought I'd share.\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.4767096159607096, 0.6559426160817835])\n",
      "\n",
      "#  7\u001b[33m ==> which may or may not be related to this project, which I also don't really understand\u001b[0m\n",
      "\t inserted to #0 : len(3) < 10 and distances([0.36919351772293035, 0.67642234306249216, 0.79157760921931775])\n",
      "\n",
      "#  8\u001b[33m ==> starting to work on this now FINALLY\u001b[0m\n",
      "\t inserted to #0 : len(4) < 10 and distances([0.44795062103655997, 0.72350617224004732, 0.58629093576527547, 0.70281477576113038])\n",
      "\n",
      "#  9\u001b[33m ==> agreed <@U2C9M9GP5> from the reading I have done I am siding with the RTM API\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(5, 3), (2, 0), (1, 0), (1, 0)])\n",
      "\n",
      "# 10\u001b[33m ==> `open question`: what is a partition of a log for kafka? is this similar to the partitioning that happens in the MapReduce framework?\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.51486277451014895])\n",
      "\n",
      "# 11\u001b[33m ==> ```Log Aggregation\n",
      "\n",
      "Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.```\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.63210357154046548, 0.73554032578735018])\n",
      "\n",
      "# 12\u001b[33m ==> <@U2C9M9GP5> sounds like we may be able to have Kafka listen to a lightweight producer tied directly to slack? rather than Flume + Kafka\u001b[0m\n",
      "\t inserted to #0 : len(3) < 10 and distances([0.50129650027407946, 0.57633187293142263, 0.76226412211146621])\n",
      "\n",
      "# 13\u001b[33m ==> My understanding is that they are indeed something similar \u001b[0m\n",
      "\t inserted to #0 : len(4) < 10 and distances([0.44693216796452478, 0.63892830155052349, 0.70534013902503589, 0.57843813673814326])\n",
      "\n",
      "# 14\u001b[33m ==> You keep separate queues for different types of messages\u001b[0m\n",
      "\t inserted to #0 : len(5) < 10 and distances([0.44639014234388386, 0.57823940607858559, 0.77054178773835169, 0.62933634744317135, 0.71028705693203165])\n",
      "\n",
      "# 15\u001b[33m ==> Yeah that's my understanding as well. A particular topic is guaranteed to go to a particular partition. And then each partition passively replicates to the number of nodes equal to the replication factor.\u001b[0m\n",
      "\t inserted to #0 : len(6) < 10 and distances([0.44366678107297647, 0.72398624698681835, 0.73832256868468149, 0.62133987345714137, 0.73356336103524722, 0.66961621515805547])\n",
      "\n",
      "# 16\u001b[33m ==> My mental model is still pretty fuzzy though. Something like Slack RTM Api &gt;&gt; ??? &gt;&gt; Kafka\u001b[0m\n",
      "\t inserted to #0 : len(7) < 10 and distances([0.5131719766861853, 0.60922162801582669, 0.71163423061183417, 0.7321326742345774, 0.54776936306487511, 0.48954203281170283, 0.55986400933429403])\n",
      "\n",
      "# 17\u001b[33m ==> The particulars of the ??? as still hazy. It seems like we should be able to take incoming messages and send them along as messages to the kafka server. But how do we do this in a resilient and scalable way I'm not quiet sure.\u001b[0m\n",
      "\t inserted to #0 : len(8) < 10 and distances([0.47289822270976911, 0.5822100601649991, 0.79952950231727649, 0.77741803490114525, 0.64937991745986257, 0.72222051948460109, 0.6541610769393531, 0.68509132340042922])\n",
      "\n",
      "# 18\u001b[33m ==> We would need some sort of serialization in the middle right?\u001b[0m\n",
      "\t inserted to #0 : len(9) < 10 and distances([0.47756466260688063, 0.6677750360947966, 0.70427161728208154, 0.75222467682886329, 0.72545523187623251, 0.63819401595581149, 0.75873412710629795, 0.60956454511319491, 0.73003792829579972])\n",
      "\n",
      "# 19\u001b[33m ==> Either Flume or Avro both apache projects \u001b[0m\n",
      "\t No similar topics (new 0) scores:([(10, 0), (5, 0), (2, 0), (1, 0), (1, 0)])\n",
      "\n",
      "# 20\u001b[33m ==> Indeed I have seen those projects mentioned for this purpose. Can you give me a 10000 foot explanation of what 'serialization' mean?\u001b[0m\n",
      "\t inserted to #2 : len(5) < 10 and distances([0.40824993660274467, 0.64402996653921918, 0.61079099490500166, 0.75466280329837621, 0.776670606735268])\n",
      "\n",
      "# 21\u001b[33m ==> Again... My understanding here...  But serialization for me just means standardizing into an object that will be readable and has follows some (usually java) format\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(6, 4), (1, 0), (10, 9), (2, 0), (1, 0), (1, 0)])\n",
      "\n",
      "# 22\u001b[33m ==> okay so JSON is a serialization\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.51307060858976083])\n",
      "\n",
      "# 23\u001b[33m ==> Basically, pickle is a serializer for python objects \u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.46802098268440662, 0.32118337847136891])\n",
      "\n",
      "# 24\u001b[33m ==> as well as many other well known formats\u001b[0m\n",
      "\t inserted to #0 : len(3) < 10 and distances([0.58456109949091772, 0.16949531381232907, 0.40362016822657015])\n",
      "\n",
      "# 25\u001b[33m ==> That is a question I have... But I would say JSON can serve as the serialized final format\u001b[0m\n",
      "\t inserted to #0 : len(4) < 10 and distances([0.7335960110648202, 0.42799379927020431, 0.35556377717969201, 0.66759583196660033])\n",
      "\n",
      "# 26\u001b[33m ==> Although there might be some additional things as text encoding\u001b[0m\n",
      "\t inserted to #0 : len(5) < 10 and distances([0.71273707284402976, 0.25130027498114171, 0.51289860357272021, 0.65988093594073338, 0.71164461804260115])\n",
      "\n",
      "# 27\u001b[33m ==> Like what happens with accents and other non-ASCII chars\u001b[0m\n",
      "\t inserted to #1 : len(6) < 10 and distances([0.51235431339018911, 0.49162986381045848, 0.54299683808055177, 0.51205120641987645, 0.3976298309253411, 0.45269847104229977])\n",
      "\n",
      "# 28\u001b[33m ==> yeah serialization tends to be the transformation of data to a standard format\u001b[0m\n",
      "\t inserted to #0 : len(7) < 10 and distances([0.47136431398156875, 0.59563267838902401, 0.55817941656421499, 0.59732753451385756, 0.60485940276210348, 0.73041772058772303, 0.44309464458936332])\n",
      "\n",
      "# 29\u001b[33m ==> ^^^ long but vvv worth the read\u001b[0m\n",
      "\t inserted to #0 : len(8) < 10 and distances([0.48770067809575529, 0.56528651158963905, 0.56793930975132201, 0.60040709615752441, 0.63561787863776953, 0.65177769239636407, 0.30348691991178434, 0.56823839987272851])\n",
      "\n",
      "# 30\u001b[33m ==> Still working through it, but this blog post seems pertinent to our usecase\u001b[0m\n",
      "\t inserted to #0 : len(9) < 10 and distances([0.40659630153377063, 0.51458849118389349, 0.53068202912193174, 0.58913777896751296, 0.63664489381039335, 0.60837278772744485, 0.28183957297595097, 0.56141729147033537, 0.57620794447944834])\n",
      "\n",
      "... Done, processed 30 messages\n"
     ]
    }
   ],
   "source": [
    "# Need to call .get_messages each time, because if not the message_stream will have \"dried out\"\n",
    "msg_stream = awwdb.get_messages(type_of_query='day', periods=5, channel='tech-stuff', min_words=5)\n",
    "\n",
    "window_us = classify_stream(msg_stream, distance=dist_m2m, low_threshold=.4, high_threshold=.7, low_step=.05, high_step=.02, max_messages=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #7 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 10 \n",
      "  Topic # 1  --> size: 6  \n",
      "  Topic # 2  --> size: 1  \n",
      "  Topic # 3  --> size: 10 \n",
      "  Topic # 4  --> size: 2  \n",
      "  Topic # 5  --> size: 1  \n",
      "  Topic # 6  --> size: 1  \n"
     ]
    }
   ],
   "source": [
    "inspect_window(window_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- No similar topics (to 0) scores:([(5, 3), (2, 0), (1, 0), (1, 0)])\n",
      "\t\u001b[33magreed <@U2C9M9GP5> from the reading I have done I am siding with the RTM API\u001b[0m\n",
      "\n",
      "\n",
      "1 -- len(1) < 3 and distances([0.51486277451014895])\n",
      "\t\u001b[33m`open question`: what is a partition of a log for kafka? is this similar to the partitioning that happens in the MapReduce framework?\u001b[0m\n",
      "\n",
      "\n",
      "2 -- len(2) < 3 and distances([0.63210357154046548, 0.73554032578735018])\n",
      "\t\u001b[33m```Log Aggregation\n",
      "\n",
      "Many people use Kafka as a replacement for a log aggregation solution. Log aggregation typically collects physical log files off servers and puts them in a central place (a file server or HDFS perhaps) for processing. Kafka abstracts away the details of files and gives a cleaner abstraction of log or event data as a stream of messages. This allows for lower-latency processing and easier support for multiple data sources and distributed data consumption. In comparison to log-centric systems like Scribe or Flume, Kafka offers equally good performance, stronger durability guarantees due to replication, and much lower end-to-end latency.```\u001b[0m\n",
      "\n",
      "\n",
      "3 -- len(3) < 10 and distances([0.50129650027407946, 0.57633187293142263, 0.76226412211146621])\n",
      "\t\u001b[33m<@U2C9M9GP5> sounds like we may be able to have Kafka listen to a lightweight producer tied directly to slack? rather than Flume + Kafka\u001b[0m\n",
      "\n",
      "\n",
      "4 -- len(4) < 10 and distances([0.44693216796452478, 0.63892830155052349, 0.70534013902503589, 0.57843813673814326])\n",
      "\t\u001b[33mMy understanding is that they are indeed something similar \u001b[0m\n",
      "\n",
      "\n",
      "5 -- len(5) < 10 and distances([0.44639014234388386, 0.57823940607858559, 0.77054178773835169, 0.62933634744317135, 0.71028705693203165])\n",
      "\t\u001b[33mYou keep separate queues for different types of messages\u001b[0m\n",
      "\n",
      "\n",
      "6 -- len(6) < 10 and distances([0.44366678107297647, 0.72398624698681835, 0.73832256868468149, 0.62133987345714137, 0.73356336103524722, 0.66961621515805547])\n",
      "\t\u001b[33mYeah that's my understanding as well. A particular topic is guaranteed to go to a particular partition. And then each partition passively replicates to the number of nodes equal to the replication factor.\u001b[0m\n",
      "\n",
      "\n",
      "7 -- len(7) < 10 and distances([0.5131719766861853, 0.60922162801582669, 0.71163423061183417, 0.7321326742345774, 0.54776936306487511, 0.48954203281170283, 0.55986400933429403])\n",
      "\t\u001b[33mMy mental model is still pretty fuzzy though. Something like Slack RTM Api &gt;&gt; ??? &gt;&gt; Kafka\u001b[0m\n",
      "\n",
      "\n",
      "8 -- len(8) < 10 and distances([0.47289822270976911, 0.5822100601649991, 0.79952950231727649, 0.77741803490114525, 0.64937991745986257, 0.72222051948460109, 0.6541610769393531, 0.68509132340042922])\n",
      "\t\u001b[33mThe particulars of the ??? as still hazy. It seems like we should be able to take incoming messages and send them along as messages to the kafka server. But how do we do this in a resilient and scalable way I'm not quiet sure.\u001b[0m\n",
      "\n",
      "\n",
      "9 -- len(9) < 10 and distances([0.47756466260688063, 0.6677750360947966, 0.70427161728208154, 0.75222467682886329, 0.72545523187623251, 0.63819401595581149, 0.75873412710629795, 0.60956454511319491, 0.73003792829579972])\n",
      "\t\u001b[33mWe would need some sort of serialization in the middle right?\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topic(window_us[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convert into a `nlp.text.window.Window`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp.text import topic as gt\n",
    "from nlp.text import window as gw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old _topic list_ (let's check we get the same results after converting it into a window...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #7 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 10 \n",
      "  Topic # 1  --> size: 6  \n",
      "  Topic # 2  --> size: 1  \n",
      "  Topic # 3  --> size: 10 \n",
      "  Topic # 4  --> size: 2  \n",
      "  Topic # 5  --> size: 1  \n",
      "  Topic # 6  --> size: 1  \n"
     ]
    }
   ],
   "source": [
    "inspect_window(window_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_window = gw.from_topic_list(window_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #7 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 10 \n",
      "  Topic # 1  --> size: 6  \n",
      "  Topic # 2  --> size: 1  \n",
      "  Topic # 3  --> size: 10 \n",
      "  Topic # 4  --> size: 2  \n",
      "  Topic # 5  --> size: 1  \n",
      "  Topic # 6  --> size: 1  \n"
     ]
    }
   ],
   "source": [
    "real_window.report_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- No similar topics (to 0) scores:([(6, 4), (1, 0), (10, 9), (2, 0), (1, 0), (1, 0)])\n",
      "\t\u001b[33mAgain... My understanding here...  But serialization for me just means standardizing into an object that will be readable and has follows some (usually java) format\u001b[0m\n",
      "\n",
      "\n",
      "1 -- len(1) < 3 and distances([0.51307060858976083])\n",
      "\t\u001b[33mokay so JSON is a serialization\u001b[0m\n",
      "\n",
      "\n",
      "2 -- len(2) < 3 and distances([0.46802098268440662, 0.32118337847136891])\n",
      "\t\u001b[33mBasically, pickle is a serializer for python objects \u001b[0m\n",
      "\n",
      "\n",
      "3 -- len(3) < 10 and distances([0.58456109949091772, 0.16949531381232907, 0.40362016822657015])\n",
      "\t\u001b[33mas well as many other well known formats\u001b[0m\n",
      "\n",
      "\n",
      "4 -- len(4) < 10 and distances([0.7335960110648202, 0.42799379927020431, 0.35556377717969201, 0.66759583196660033])\n",
      "\t\u001b[33mThat is a question I have... But I would say JSON can serve as the serialized final format\u001b[0m\n",
      "\n",
      "\n",
      "5 -- len(5) < 10 and distances([0.71273707284402976, 0.25130027498114171, 0.51289860357272021, 0.65988093594073338, 0.71164461804260115])\n",
      "\t\u001b[33mAlthough there might be some additional things as text encoding\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "real_window.topics[1].report_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Window in a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../nlp/data/windows/alex_new_config_window.pk', 'wb') as f:\n",
    "    pk.dump(real_window, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done... Now we can use it for visualization purposes outside &#9786;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
