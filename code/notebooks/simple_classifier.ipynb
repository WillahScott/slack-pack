{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The below cell will expand the notebook width to the (almost - 95%) full width of the browser_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "document.getElementById('notebook-container').style.width = '95%'"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "document.getElementById('notebook-container').style.width = '95%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the path to the slack-pack/code/ folder in order to be able to import nlp module\n",
    "import sys, os\n",
    "\n",
    "NLP_PATH = '/'.join(os.path.abspath('.').split('/')[:-1]) + '/'\n",
    "sys.path.append(NLP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from nlp.text import extractor as xt\n",
    "\n",
    "from nlp.geometry import repr as gr\n",
    "from nlp.geometry import dist as gd\n",
    "from nlp.grammar import tokenizer as gt\n",
    "from nlp.text import window as gw\n",
    "\n",
    "from nlp.models import similarity_calculation as gsc\n",
    "from nlp.models import message_classification as gmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different representations\n",
    "\n",
    "We need to load the different representations (we will use `nlp.geometry.repr.GloVe` class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glove.6B.100d.txt', 'glove.6B.300d.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.list_corpora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 514 ms, total: 13.1 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the GloVe representation\n",
    "glove100_rep = gr.GloVe('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.3 s, sys: 1.35 s, total: 33.6 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the GloVe representation\n",
    "glove300_rep = gr.GloVe('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance function\n",
    "\n",
    "The following function defines a distance between to texts (it first cleans them using `nlp.grammar.tokenizer.SimpleCleaner`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean = gt.SimpleCleaner()\n",
    "\n",
    "def dist_m2m(m1, m2):\n",
    "    # tokenize\n",
    "    text1 = clean(m1.lower())\n",
    "    text2 = clean(m2.lower())\n",
    "\n",
    "    # get geometric representation\n",
    "    rep1 = glove100_rep(text1)\n",
    "    rep2 = glove100_rep(text2)\n",
    "    \n",
    "    return gd.cosine(rep1, rep2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions for inspecting the outputted window (topic list)\n",
    "\n",
    "With `inspect_window` we get a list of the topics and the #messages in each\n",
    "\n",
    "With `print_topic` we get all the messages in the given topic, along with the reason why they were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspect_window(window):\n",
    "    print( 'Window has #{} topics\\n'.format( len(window) ) )\n",
    "    \n",
    "    print( 'Topic length report:' )\n",
    "    for i, tpc in enumerate(window):\n",
    "        print( '  Topic #{:>2}  --> size: {:<3}'.format(i, len(tpc)) )\n",
    "\n",
    "def print_topic(topic):\n",
    "    for i,(m,r) in enumerate(topic):\n",
    "        print '{} -- {}\\n\\t{}\\n\\n'.format(i,r,m.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classifier\n",
    "\n",
    "The main classifying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_stream(message_stream, max_messages=20, low_threshold=.7, high_threshold=.85, low_step=.05, high_step=.02, verbose=True):\n",
    "    topics = []\n",
    "    for m, msg in enumerate(message_stream):\n",
    "        if m > max_messages:\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print '#{:>3}\\033[33m ==> {}\\033[0m'.format(m, msg.text)\n",
    "\n",
    "        if len(topics) == 0:\n",
    "            topics.insert(0, [(msg, 'First message')] )\n",
    "            if verbose:\n",
    "                print '\\t First message (new 0)\\n'\n",
    "\n",
    "        else:\n",
    "            # We will sequentially try to append to each topic ...\n",
    "            #    as time goes by it is harder to append to a topic\n",
    "\n",
    "            low_th = low_threshold\n",
    "            high_th = high_threshold\n",
    "            topic_scores = []  # in case no topic is close\n",
    "\n",
    "            for t in xrange(len(topics)):\n",
    "                tp_len = len(topics[t])\n",
    "                distances = map(lambda x: dist_m2m(msg.text, x[0].text), topics[t])\n",
    "\n",
    "                # Assign a non-linear score (very close messages score higher)\n",
    "                score = sum([ 0 if d < low_th else 1 if d < high_th else 3 for d in distances ])\n",
    "\n",
    "                # Very large topics (> 10) should be harder to append to,\n",
    "                #   since the odds of a casual match are higher\n",
    "                if (tp_len < 3) and (score > 0):\n",
    "                    reason = 'len({}) < 3 and distances({})'.format(tp_len, distances)\n",
    "                    _topic = topics.pop(t)  # pop from topic queue\n",
    "                    _topic.append( (msg, reason) )\n",
    "                    topics.insert(0, _topic)  # append to first topic\n",
    "                    if verbose:\n",
    "                        print '\\t inserted to #{} : {}\\n'.format(t, reason)\n",
    "                    break\n",
    "\n",
    "                elif (tp_len < 10) and (score > 10):\n",
    "                    reason = 'len({}) < 10 and distances({})'.format(tp_len, distances)\n",
    "                    _topic = topics.pop(t)  # pop from topic queue\n",
    "                    _topic.append( (msg, 'len({}) < 10 and distances({})'.format(tp_len, distances)) )\n",
    "                    topics.insert(0, _topic)  # append to first topic\n",
    "                    if verbose:\n",
    "                        print '\\t inserted to #{} : {}\\n'.format(t, reason)\n",
    "                    break\n",
    "\n",
    "                elif (tp_len > 10) and (score > tp_len*1.5):\n",
    "                    reason = 'len({}) > 10 and distances({})'.format(tp_len, distances)\n",
    "                    _topic = topics.pop(t)  # pop from topic queue\n",
    "                    _topic.append( (msg, 'len({}) > 10 and distances({})'.format(tp_len, distances)) )\n",
    "                    topics.insert(0, _topic)  # append to first topic\n",
    "                    if verbose:\n",
    "                        print '\\t inserted to #{} : {}\\n'.format(t, reason)\n",
    "                    break\n",
    "\n",
    "                topic_scores.append( (tp_len,score) )  # append score to topic_scores\n",
    "\n",
    "                # else try with next topic --> harder\n",
    "                low_th += low_step if low_th+low_step < high_th else high_step\n",
    "                high_th += high_step\n",
    "            else:\n",
    "                # If no topic was suitable --> Start new topic\n",
    "                topics.insert(0, [(msg, 'No similar topics (to 0) scores:({})'.format(topic_scores))] )\n",
    "                if verbose:\n",
    "                    print '\\t No similar topics (new 0) scores:({})\\n'.format(topic_scores)\n",
    "\n",
    "    print '... Done, processed {} messages'.format(m)\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the extractor (JSON or Cassandra)\n",
    "awwdb = xt.CassandraExtractor(cluster_ips=['54.175.189.47'],\n",
    "                              session_keyspace='test_keyspace',\n",
    "                              table_name='awaybot_messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to call .get_messages each time, because if not the message_stream will have \"dried out\"\n",
    "msg_stream = awwdb.get_messages(type_of_query='hour', channel='general')\n",
    "\n",
    "window_us = classify_stream(msg_stream, max_messages=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0\u001b[33m ==> Good news is that i found a more straightforward way, and that it is better performing. Double win.\u001b[0m\n",
      "\t First message (new 0)\n",
      "\n",
      "#  1\u001b[33m ==> <@U16S9N0LE|nvisal> has joined the channel\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(1, 0)])\n",
      "\n",
      "#  2\u001b[33m ==> hit that shit with a %dopar%\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(1, 0), (1, 0)])\n",
      "\n",
      "#  3\u001b[33m ==> <@U16S1V6LX|kristineeck> has joined the channel\u001b[0m\n",
      "\t inserted to #1 : len(1) < 3 and distances([1.0])\n",
      "\n",
      "#  4\u001b[33m ==> <@U16RAECF5|micah.gr> has joined the channel\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([1.0, 1.0])\n",
      "\n",
      "#  5\u001b[33m ==> I love me some lapply()\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(3, 0), (1, 0), (1, 0)])\n",
      "\n",
      "#  6\u001b[33m ==> <@U16TY5M6F|cjfariss> has joined the channel\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(1, 0), (3, 9), (1, 0), (1, 0)])\n",
      "\n",
      "#  7\u001b[33m ==> <@U16RLTH3N|alex> has joined the channel\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([1.0])\n",
      "\n",
      "#  8\u001b[33m ==> <@U16RY7PR6|cdcrabtree> has joined the channel\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([1.0, 1.0])\n",
      "\n",
      "#  9\u001b[33m ==> But sometimes I like to take my time with some for() \u001b[0m\n",
      "\t No similar topics (new 0) scores:([(3, 0), (1, 0), (3, 0), (1, 0), (1, 0)])\n",
      "\n",
      "# 10\u001b[33m ==> Likelihood  I have ANY idea what this is doing tomorrow is approaching zero. ```\n",
      "unlist(\n",
      "  lapply(\n",
      "    lapply(random.alter.distance, function(x) {\n",
      "      apply(x, 1, mean, na.rm = TRUE)\n",
      "    }), \n",
      "  mean, na.rm = TRUE)\n",
      ")\n",
      "```\u001b[0m\n",
      "\t No similar topics (new 0) scores:([(1, 0), (3, 0), (1, 0), (3, 0), (1, 0), (1, 0)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_stream = casdb.get_messages(type_of_query='hour', channel='random', min_words=5)\n",
    "\n",
    "window_10 = classify_stream(message_stream, max_messages=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #7 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 1  \n",
      "  Topic # 1  --> size: 1  \n",
      "  Topic # 2  --> size: 3  \n",
      "  Topic # 3  --> size: 1  \n",
      "  Topic # 4  --> size: 3  \n",
      "  Topic # 5  --> size: 1  \n",
      "  Topic # 6  --> size: 1  \n"
     ]
    }
   ],
   "source": [
    "inspect_window(window_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- No similar topics (to 0) scores:([(1, 0)])\n",
      "\t<@U16S9N0LE|nvisal> has joined the channel\n",
      "\n",
      "\n",
      "1 -- len(1) < 3 and distances([1.0])\n",
      "\t<@U16S1V6LX|kristineeck> has joined the channel\n",
      "\n",
      "\n",
      "2 -- len(2) < 3 and distances([1.0, 1.0])\n",
      "\t<@U16RAECF5|micah.gr> has joined the channel\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topic(window_10[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Done, processed 119 messages\n",
      "... Done, processed 119 messages\n",
      "... Done, processed 119 messages\n",
      "... Done, processed 119 messages\n",
      "1 loop, best of 3: 487 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "message_stream = casdb.get_messages(type_of_query='hour', channel='general', min_words=5)\n",
    "\n",
    "full_window = classify_stream(message_stream, max_messages=10000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #60 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 2  \n",
      "  Topic # 1  --> size: 1  \n",
      "  Topic # 2  --> size: 1  \n",
      "  Topic # 3  --> size: 2  \n",
      "  Topic # 4  --> size: 3  \n",
      "  Topic # 5  --> size: 1  \n",
      "  Topic # 6  --> size: 1  \n",
      "  Topic # 7  --> size: 2  \n",
      "  Topic # 8  --> size: 3  \n",
      "  Topic # 9  --> size: 3  \n",
      "  Topic #10  --> size: 1  \n",
      "  Topic #11  --> size: 3  \n",
      "  Topic #12  --> size: 1  \n",
      "  Topic #13  --> size: 3  \n",
      "  Topic #14  --> size: 3  \n",
      "  Topic #15  --> size: 3  \n",
      "  Topic #16  --> size: 1  \n",
      "  Topic #17  --> size: 1  \n",
      "  Topic #18  --> size: 3  \n",
      "  Topic #19  --> size: 1  \n",
      "  Topic #20  --> size: 1  \n",
      "  Topic #21  --> size: 1  \n",
      "  Topic #22  --> size: 3  \n",
      "  Topic #23  --> size: 1  \n",
      "  Topic #24  --> size: 1  \n",
      "  Topic #25  --> size: 3  \n",
      "  Topic #26  --> size: 3  \n",
      "  Topic #27  --> size: 2  \n",
      "  Topic #28  --> size: 3  \n",
      "  Topic #29  --> size: 1  \n",
      "  Topic #30  --> size: 3  \n",
      "  Topic #31  --> size: 1  \n",
      "  Topic #32  --> size: 1  \n",
      "  Topic #33  --> size: 3  \n",
      "  Topic #34  --> size: 3  \n",
      "  Topic #35  --> size: 1  \n",
      "  Topic #36  --> size: 1  \n",
      "  Topic #37  --> size: 1  \n",
      "  Topic #38  --> size: 3  \n",
      "  Topic #39  --> size: 1  \n",
      "  Topic #40  --> size: 3  \n",
      "  Topic #41  --> size: 1  \n",
      "  Topic #42  --> size: 1  \n",
      "  Topic #43  --> size: 3  \n",
      "  Topic #44  --> size: 3  \n",
      "  Topic #45  --> size: 3  \n",
      "  Topic #46  --> size: 3  \n",
      "  Topic #47  --> size: 1  \n",
      "  Topic #48  --> size: 1  \n",
      "  Topic #49  --> size: 2  \n",
      "  Topic #50  --> size: 2  \n",
      "  Topic #51  --> size: 3  \n",
      "  Topic #52  --> size: 3  \n",
      "  Topic #53  --> size: 3  \n",
      "  Topic #54  --> size: 1  \n",
      "  Topic #55  --> size: 3  \n",
      "  Topic #56  --> size: 1  \n",
      "  Topic #57  --> size: 1  \n",
      "  Topic #58  --> size: 3  \n",
      "  Topic #59  --> size: 3  \n"
     ]
    }
   ],
   "source": [
    "inspect_window(full_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  0\u001b[33m ==> Its a pretty simple recommender -- seems to just key on most descriptive word. This is a little tough, because while there are a lot of nerds who rate beer, there are a lot of bros who know like 4 beer words: dank, fruit, sour, ... . And, this sort of mainlines the recommendations. I think a little filtering would help clean this up.\u001b[0m\n",
      "\t First message (to 0)\n",
      "\n",
      "#  1\u001b[33m ==> Two syllabi that my be of interest: Matt Blackwell: <http://www.mattblackwell.org/files/teaching/gov2002-15f-syllabus.pdf> and Danny Hidalgo: <http://www.mit.edu/~dhidalgo/syllabi/17_802_syll2014.pdf>.  Alex pointed out that Hidalgo is more interested in observational data. Nice bibliography between the two.\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.5085626196861307])\n",
      "\n",
      "#  2\u001b[33m ==> Does anybody have an in either at Pew or the NCSL? I wonder because theyve published a _Stateline_ series, which at least has the ethnicity data that Micah and Charles were thinking about. Ive been warned that if I scrape the data from the group that I had posted about last week, that the group would shut down campus access and I would be persona non grata.\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.76491236701813414, 0.59694082148858452])\n",
      "\n",
      "#  3\u001b[33m ==> Haiti project is humming right along \u001b[0m\n",
      "\t No similar topics (to 0) scores:([(3, 2)])\n",
      "\n",
      "#  4\u001b[33m ==> <@U16TY5M6F>: How did that USAID boondoggle in Haiti end up going for you?\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.79811211131722659])\n",
      "\n",
      "#  5\u001b[33m ==> On helping people understand the value of experimental research: Ive been thinking a lot about the push back that Alex and I got in designing a useful intervention. Today again I was talking to someone who has the opportunity to learn about a programs effectiveness, but instead is waving their hands talking nonsense. Has anyone seen a good attempt at explaining the value of experimental research to a non-technical audience?\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.73785798911005129, 0.63265542408290942])\n",
      "\n",
      "#  6\u001b[33m ==> Checked it out.  Im into it.\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(3, 0), (3, 0)])\n",
      "\n",
      "#  7\u001b[33m ==> Totally! Do you have a place to stay?\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(1, 0), (3, 3), (3, 2)])\n",
      "\n",
      "#  8\u001b[33m ==> In addition, because none of this is associated with our university addresses, we are not subject to FOIAs, so we can say dumb things without the threat of being taken down for it.\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.71525056462601533])\n",
      "\n",
      "#  9\u001b[33m ==> <@U16RLTH3N|alex> uploaded a file: <https://experimentsboondoggle.slack.com/files/alex/F2BPUHUN6/gpo-pictdir-114.pdf|GPO-PICTDIR-114.pdf>\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(2, 0), (1, 0), (3, 0), (3, 0)])\n",
      "\n",
      "# 10\u001b[33m ==> a really interesting piece on design of experiments, courtesy of devesh: <https://www.povertyactionlab.org/sites/default/files/publications/20160401handbookExperimentalDesign.pdf>\u001b[0m\n",
      "\t inserted to #1 : len(2) < 3 and distances([0.45565831539834795, 0.63194875587559229])\n",
      "\n",
      "# 11\u001b[33m ==> (BTW: I dont know what IIA is...)\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(3, 0), (1, 0), (1, 0), (3, 0), (3, 0)])\n",
      "\n",
      "# 12\u001b[33m ==> I bet that we could even get them to task an intern onto collecting the corpus of text for us as well.\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(1, 0), (3, 1), (1, 0), (1, 0), (3, 0), (3, 0)])\n",
      "\n",
      "# 13\u001b[33m ==> Wait. It is still necessary to buy postage for mail in ballots in the general election.\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.65662776503482612])\n",
      "\n",
      "# 14\u001b[33m ==> <@U16RLTH3N> Im available all day except from 12-3. Itd be great to get together.\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.54693858585632982, 0.60431863323633384])\n",
      "\n",
      "# 15\u001b[33m ==> Wow; that study you linked to is some BULLSHIT.\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(3, 1), (1, 0), (3, 0), (1, 0), (1, 0), (3, 0), (3, 0)])\n",
      "\n",
      "# 16\u001b[33m ==> Probably. Whats the underlying concept?\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.57495194201925193])\n",
      "\n",
      "# 17\u001b[33m ==> This study is that _straight_ dope. But I want a <@U16TY5M6F> viewpoint on their model. In particular, theyre interpreting their item difficulty parameters in a _really_ literal sense (literally the probability that a traffic stop will occur). Id love to read closely and talk with anyone who has also read it closely, though I know that you are all probably at APSA right now.\u001b[0m\n",
      "\t inserted to #0 : len(2) < 3 and distances([0.64112213674947094, 0.75298928849356406])\n",
      "\n",
      "# 18\u001b[33m ==> Im asking my research librarian if he knows of other such resources. One possibility is the Congressional YellowBook.\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(3, 3), (3, 2), (1, 0), (3, 0), (1, 0), (1, 0), (3, 0), (3, 0)])\n",
      "\n",
      "# 19\u001b[33m ==> <@U16RAECF5>: Also, Im working out responses to your emails. One of them is almost perfectly in line with something that I just spoke with Matt Golder about earlier today. It was like you read my mind. Spooky.\u001b[0m\n",
      "\t inserted to #0 : len(1) < 3 and distances([0.74033968508354087])\n",
      "\n",
      "# 20\u001b[33m ==> <@U16RLTH3N>: yepski. give me a call whenevs.\u001b[0m\n",
      "\t No similar topics (to 0) scores:([(2, 0), (3, 0), (3, 0), (1, 0), (3, 0), (1, 0), (1, 0), (3, 0), (3, 0)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message_stream = casdb.get_messages(type_of_query='hour', channel='general', min_words=5)\n",
    "\n",
    "window_20 = classify_stream(message_stream, max_messages=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #10 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 1  \n",
      "  Topic # 1  --> size: 2  \n",
      "  Topic # 2  --> size: 3  \n",
      "  Topic # 3  --> size: 3  \n",
      "  Topic # 4  --> size: 1  \n",
      "  Topic # 5  --> size: 3  \n",
      "  Topic # 6  --> size: 1  \n",
      "  Topic # 7  --> size: 1  \n",
      "  Topic # 8  --> size: 3  \n",
      "  Topic # 9  --> size: 3  \n"
     ]
    }
   ],
   "source": [
    "inspect_window(window_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message-to-topic distance\n",
    "\n",
    "When we measure the distance of a message to a topic we are doing som clustering..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Window on which we'll store the topics\n",
    "topic_window = gw.Window(window_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the similar topic calculator\n",
    "simtop = gsc.SimilarTopicCalculator(representation=glove100_rep,\n",
    "                                    similarity=gd.cosine,\n",
    "                                    tokenizer=gt.SimpleCleaner())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the message classifier\n",
    "classifier = gmc.MessageClassifier(window=topic_window,\n",
    "                                   similarity_threshold=0.4,\n",
    "                                   similar_topic_calculator=simtop,\n",
    "                                   reply_analysis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ... Finished classifying 120 messages\n"
     ]
    }
   ],
   "source": [
    "# Classify the message as we obtain them from the stream\n",
    "message_stream = casdb.get_messages(type_of_query='hour', channel='general')\n",
    "classifier.classify_stream(message_stream=message_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inspect_window(window):\n",
    "    print( 'Window has #{} topics\\n'.format( len(window) ) )\n",
    "    \n",
    "    print( 'Topic length report:' )\n",
    "    for i, tpc in enumerate(window.topics):\n",
    "        print( '  Topic #{:>2}  --> size: {:<3}'.format(i, len(tpc)) )\n",
    "\n",
    "def print_topic(topic):\n",
    "    for i,m,r in zip(range(len(topic)), topic.messages, topic.reasons):\n",
    "        print '{} -- {}\\n\\t{}\\n\\n'.format(i,r,m.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window has #3 topics\n",
      "\n",
      "Topic length report:\n",
      "  Topic # 0  --> size: 2  \n",
      "  Topic # 1  --> size: 7  \n",
      "  Topic # 2  --> size: 111\n"
     ]
    }
   ],
   "source": [
    "inspect_window(topic_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
